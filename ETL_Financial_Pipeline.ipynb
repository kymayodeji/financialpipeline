{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# End-To-End ETL Financial Pipline \n",
    "## Utilizing Python, APIs, Web Scraping and SQL to consolidate and process financial data from multiple types of sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Collecting TextBlob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from TextBlob) (3.8.1)\n",
      "Requirement already satisfied: click in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from nltk>=3.1->TextBlob) (4.65.0)\n",
      "Installing collected packages: TextBlob\n",
      "Successfully installed TextBlob-0.17.1\n",
      "Requirement already satisfied: Beautifulsoup4 in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from Beautifulsoup4) (2.4)\n",
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m817.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.7.22)\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17398 sha256=775ade5b71953515278b3c6846c322656e88c218a789233035b895234ead4c0d\n",
      "  Stored in directory: /Users/kymair/Library/Caches/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
      "Successfully built googletrans\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.4.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.4.3 requires pyqtwebengine<5.16, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
      "Requirement already satisfied: nltk in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/kymair/anaconda3/envs/analytics/lib/python3.10/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    " # Install Necessary resources \n",
    "!pip install nltk\n",
    "!pip install TextBlob\n",
    "!pip install Beautifulsoup4\n",
    "!pip install googletrans==4.0.0-rc1\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Needed Libraries and Necessary resources from NLTK, Beautiful Soup, TextBlob, Google Translate\n",
    "#import html5lib\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from googletrans import Translator \n",
    "import os\n",
    "import sqlite3 as sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1a) Fetch Financial News headlines\n",
    "cnbc_URL = \"https://www.cnbc.com/stocks/\"\n",
    "cnbc_headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'}\n",
    "cnbc_r = requests.get(cnbc_URL, headers=cnbc_headers)\n",
    "cnbc_timestamp=datetime.now()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1b)Fetch Stock Market Data \n",
    "aapl_url='https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=AAPL&interval=60min&outputsize=compact&apikey=LJ7H7F9D9Z7ECUK7&datatype=csv'\n",
    "r_aapl=requests.get(aapl_url)\n",
    "msft_url='https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=MSFT&interval=60min&outputsize=compact&apikey=LJ7H7F9D9Z7ECUK7&datatype=csv'\n",
    "r_msft=requests.get(msft_url)\n",
    "goog_url='https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=GOOG&interval=60min&outputsize=compact&apikey=LJ7H7F9D9Z7ECUK7&datatype=csv'\n",
    "r_goog=requests.get(goog_url)\n",
    "amzn_url='https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=AMZN&interval=60min&outputsize=compact&apikey=LJ7H7F9D9Z7ECUK7&datatype=csv'\n",
    "r_amzn=requests.get(amzn_url)\n",
    "meta_url='https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=META&interval=60min&outputsize=compact&apikey=LJ7H7F9D9Z7ECUK7&datatype=csv'\n",
    "r_meta=requests.get(meta_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial News Headline Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the extracted Financial News headline data, retrieve the url for the headline\n",
    "html_text = cnbc_r.text\n",
    "soup = BeautifulSoup(html_text, 'lxml')\n",
    "card_titles=soup.find_all('a', class_='Card-title')\n",
    "headlines_data=[]\n",
    "for title in card_titles:\n",
    "    headlines_data.append(title['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title from url and strip .html then create a list of sentences for each url \n",
    "df_holder=pd.DataFrame(headlines_data, columns=['url'])\n",
    "df_holder[['http','empty', 'site', 'year','month','date','title']] = df_holder.url.str.split(\"/\",expand=True)\n",
    "titles=df_holder['title'].str.replace('-', ' ').tolist()\n",
    "sentences =  [x[:-4] for x in titles]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a data frame for the headlines and add the columns for title, url, timestamp\n",
    "df_headlines=pd.DataFrame(sentences, columns=['headline'])\n",
    "df_headlines['url']=df_holder['url']\n",
    "df_headlines['timestamp'] = cnbc_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2a. Add the sentiment score and sentiment for each headline sentence using the toekenize functions\n",
    "df_headlines['sentiment_score']=df_headlines['headline'].apply(lambda sent: TextBlob(sent).sentiment.polarity)\n",
    "def f(score):\n",
    "    if -1 <= score  and score < -0.2: return \"negative\"\n",
    "    elif -0.2 <= score and score > 0.2: return \"neutral\" \n",
    "    else: return \"positive\"\n",
    "assign_sentiment = lambda x: list(map(f, x))\n",
    "df_headlines['sentiment']=assign_sentiment(df_headlines['sentiment_score'])\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/kymair/nltk_data'\n    - '/Users/kymair/anaconda3/envs/analytics/nltk_data'\n    - '/Users/kymair/anaconda3/envs/analytics/share/nltk_data'\n    - '/Users/kymair/anaconda3/envs/analytics/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#2b. Tokenize the text into individual \"relevant words\" by NLTK\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_headlines[\u001b[39m'\u001b[39m\u001b[39mrelevant_words\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mdf_headlines[\u001b[39m'\u001b[39;49m\u001b[39mheadline\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(word_tokenize)\n",
      "File \u001b[0;32m~/anaconda3/envs/analytics/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/anaconda3/envs/analytics/lib/python3.10/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/envs/analytics/lib/python3.10/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/analytics/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/analytics/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/analytics/lib/python3.10/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/analytics/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/envs/analytics/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/anaconda3/envs/analytics/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/kymair/nltk_data'\n    - '/Users/kymair/anaconda3/envs/analytics/nltk_data'\n    - '/Users/kymair/anaconda3/envs/analytics/share/nltk_data'\n    - '/Users/kymair/anaconda3/envs/analytics/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#2b. Tokenize the text into individual \"relevant words\" by NLTK\n",
    "df_headlines['relevant_words']=df_headlines['headline'].apply(word_tokenize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2c. Create new translated column\n",
    "my_translator = Translator()\n",
    "df_headlines['headline_es']=df_headlines['headline'].apply(lambda x: my_translator.translate(x, src='en', dest='es').text)\n",
    "df_headlines['headline_it']=df_headlines['headline'].apply(lambda x: my_translator.translate(x, src='en', dest='it').text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>china economy july industrial output fixed ass...</td>\n",
       "      <td>https://www.cnbc.com/2023/08/15/china-economy-...</td>\n",
       "      <td>2023-08-14 21:13:22.117892</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forget china top alternatives stocks to buy ac...</td>\n",
       "      <td>https://www.cnbc.com/2023/08/15/forget-china-t...</td>\n",
       "      <td>2023-08-14 21:13:22.117892</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>with ai stocks down jim cramer says many not b...</td>\n",
       "      <td>https://www.cnbc.com/2023/08/14/with-ai-stocks...</td>\n",
       "      <td>2023-08-14 21:13:22.117892</td>\n",
       "      <td>0.214815</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cramers lightning round stay away from plug po...</td>\n",
       "      <td>https://www.cnbc.com/2023/08/14/cramers-lightn...</td>\n",
       "      <td>2023-08-14 21:13:22.117892</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to own great stock you must prepare for the lo...</td>\n",
       "      <td>https://www.cnbc.com/2023/08/14/to-own-great-s...</td>\n",
       "      <td>2023-08-14 21:13:22.117892</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline  \\\n",
       "ID                                                      \n",
       "0   china economy july industrial output fixed ass...   \n",
       "1   forget china top alternatives stocks to buy ac...   \n",
       "2   with ai stocks down jim cramer says many not b...   \n",
       "3   cramers lightning round stay away from plug po...   \n",
       "4   to own great stock you must prepare for the lo...   \n",
       "\n",
       "                                                  url  \\\n",
       "ID                                                      \n",
       "0   https://www.cnbc.com/2023/08/15/china-economy-...   \n",
       "1   https://www.cnbc.com/2023/08/15/forget-china-t...   \n",
       "2   https://www.cnbc.com/2023/08/14/with-ai-stocks...   \n",
       "3   https://www.cnbc.com/2023/08/14/cramers-lightn...   \n",
       "4   https://www.cnbc.com/2023/08/14/to-own-great-s...   \n",
       "\n",
       "                    timestamp  sentiment_score sentiment  \n",
       "ID                                                        \n",
       "0  2023-08-14 21:13:22.117892         0.100000  positive  \n",
       "1  2023-08-14 21:13:22.117892         0.500000   neutral  \n",
       "2  2023-08-14 21:13:22.117892         0.214815   neutral  \n",
       "3  2023-08-14 21:13:22.117892        -0.200000  positive  \n",
       "4  2023-08-14 21:13:22.117892         0.700000   neutral  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View Data\n",
    "df_headlines.index.name = \"ID\"\n",
    "df_headlines.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file folders (Made it manually but included the code to make it within the code)\n",
    "os.makedirs(\"data\")\n",
    "os.makedirs(\"data/headlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d. Save the headlines DataFrame to a CSV file \n",
    "df_headlines.index = df_headlines.index.rename('ID')\n",
    "df_headlines.to_csv('data/headlines/headlines_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stocks Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the stock market data \n",
    "aapl_content=r_aapl.content\n",
    "aapl = pd.read_csv(io.StringIO(aapl_content.decode('utf-8')))\n",
    "msft_content=r_msft.content\n",
    "msft = pd.read_csv(io.StringIO(msft_content.decode('utf-8')))\n",
    "goog_content=r_goog.content\n",
    "goog = pd.read_csv(io.StringIO(goog_content.decode('utf-8')))\n",
    "amzn_content=r_amzn.content\n",
    "amzn = pd.read_csv(io.StringIO(amzn_content.decode('utf-8')))\n",
    "meta_content=r_meta.content\n",
    "meta = pd.read_csv(io.StringIO(meta_content.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In prepraration of merge each stock dataframe into one, Lets add the stock name as a column named symbol \n",
    "aapl['symbol']='AAPL'\n",
    "msft['symbol']='MSFT'\n",
    "goog['symbol']='GOOG'\n",
    "amzn['symbol']='AMZN'\n",
    "meta['symbol']='META'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wn/mnlxj0jn0d771znx5hvhf3g40000gn/T/ipykernel_63456/3370357267.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_stocks.append(msft, ignore_index = True)\n",
      "/var/folders/wn/mnlxj0jn0d771znx5hvhf3g40000gn/T/ipykernel_63456/3370357267.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_stocks.append(goog, ignore_index = True)\n",
      "/var/folders/wn/mnlxj0jn0d771znx5hvhf3g40000gn/T/ipykernel_63456/3370357267.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_stocks.append(amzn, ignore_index = True)\n",
      "/var/folders/wn/mnlxj0jn0d771znx5hvhf3g40000gn/T/ipykernel_63456/3370357267.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_stocks.append(meta, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "#Merge the stocks into one data frame and rename the columns to match the assignment\n",
    "df_stocks=aapl\n",
    "df_stocks.append(msft, ignore_index = True)\n",
    "df_stocks.append(goog, ignore_index = True)\n",
    "df_stocks.append(amzn, ignore_index = True)\n",
    "df_stocks.append(meta, ignore_index = True)\n",
    "dict={\"timestamp\":\"date\", \"open\":\"open_price\", \"high\":\"highest_price\", \"low\":\"lowest_price\", \"close\":\"close_price\"}\n",
    "df_stocks.rename(columns=dict,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks.index = df_stocks.index.rename('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open_price</th>\n",
       "      <th>highest_price</th>\n",
       "      <th>lowest_price</th>\n",
       "      <th>close_price</th>\n",
       "      <th>volume</th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-14 19:00:00</td>\n",
       "      <td>179.800</td>\n",
       "      <td>179.940</td>\n",
       "      <td>179.650</td>\n",
       "      <td>179.940</td>\n",
       "      <td>47601</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-14 18:00:00</td>\n",
       "      <td>179.500</td>\n",
       "      <td>179.840</td>\n",
       "      <td>179.470</td>\n",
       "      <td>179.810</td>\n",
       "      <td>38389</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-08-14 17:00:00</td>\n",
       "      <td>179.460</td>\n",
       "      <td>204.232</td>\n",
       "      <td>162.507</td>\n",
       "      <td>179.525</td>\n",
       "      <td>47045</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-08-14 16:00:00</td>\n",
       "      <td>179.515</td>\n",
       "      <td>193.100</td>\n",
       "      <td>177.302</td>\n",
       "      <td>179.430</td>\n",
       "      <td>12804803</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-08-14 15:00:00</td>\n",
       "      <td>178.960</td>\n",
       "      <td>179.520</td>\n",
       "      <td>178.830</td>\n",
       "      <td>179.510</td>\n",
       "      <td>6373314</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date  open_price  highest_price  lowest_price  close_price  \\\n",
       "ID                                                                              \n",
       "0   2023-08-14 19:00:00     179.800        179.940       179.650      179.940   \n",
       "1   2023-08-14 18:00:00     179.500        179.840       179.470      179.810   \n",
       "2   2023-08-14 17:00:00     179.460        204.232       162.507      179.525   \n",
       "3   2023-08-14 16:00:00     179.515        193.100       177.302      179.430   \n",
       "4   2023-08-14 15:00:00     178.960        179.520       178.830      179.510   \n",
       "\n",
       "      volume symbol  \n",
       "ID                   \n",
       "0      47601   AAPL  \n",
       "1      38389   AAPL  \n",
       "2      47045   AAPL  \n",
       "3   12804803   AAPL  \n",
       "4    6373314   AAPL  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Data\n",
    "os.makedirs(\"data/stocks\")\n",
    "df_stocks.to_csv('data/stocks/stocks_data.csv', index=False,encoding='utf-8')\n",
    "df_stocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a) Create a db with the schema for the headlines table and the stocks table\n",
    "db = sql.connect('etl_extended_case.db')\n",
    "c = db.cursor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create two tables for the headline and stocks data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x158e6c940>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.execute(\"\"\"CREATE TABLE headlines(ID INTEGER PRIMARY KEY NOT NULL, \n",
    "    headline TEXT, \n",
    "    url TEXT,\n",
    "    timestamp TEXT,\n",
    "    sentiment_score REAL,\n",
    "    sentiment TEXT, \n",
    "    relevant_words TEXT,\n",
    "    headline_es TEXT, \n",
    "    headline_it TEXT)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x158e6c940>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.execute(\"\"\"CREATE TABLE stocks(ID INTEGER PRIMARY KEY NOT NULL, \n",
    "    date TEXT, \n",
    "    open_price REAL,\n",
    "    highest_price REAL,\n",
    "    lowest_price REAL,\n",
    "    close_price REAL, \n",
    "    volume INTEGER,\n",
    "    symbol TEXT)\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data from the CSV files and load into the SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfHeadlines = pd.read_csv('data/headlines/headlines_data.csv')\n",
    "dfHeadlines.to_sql('headlines', db, if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfStocks = pd.read_csv('data/stocks/stocks_data.csv')\n",
    "dfStocks.to_sql('stocks', db, if_exists='append',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x158e6c940>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.execute('''SELECT * FROM headlines''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x158e6c940>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.execute('''SELECT * from stocks''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x158e6c940>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check data\n",
    "c.execute('''SELECT * FROM headlines LIMIT 5''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x158e6c940>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.execute('''SELECT * FROM stocks LIMIT 5''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.commit()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
